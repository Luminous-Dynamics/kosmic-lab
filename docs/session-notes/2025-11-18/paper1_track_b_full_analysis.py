#!/usr/bin/env python3
"""
Publication-Ready Track B Analysis for Paper 1

Generates complete Full K-Index analysis from Track B logs,
providing all statistics needed for paper revision.

Output: JSON file with publication-ready results
"""

import numpy as np
from pathlib import Path
from collections import Counter
import json
from datetime import datetime
from scipy import stats


def compute_h1_resonant_coherence(states):
    """H1: Integration across state dimensions."""
    if len(states) < 10 or states.ndim == 1:
        return 0.5

    n_dims = states.shape[1]
    if n_dims < 2:
        return 0.5

    correlations = []
    for i in range(min(n_dims, 8)):
        for j in range(i+1, min(n_dims, 8)):
            std_i = np.std(states[:, i])
            std_j = np.std(states[:, j])
            if std_i > 1e-10 and std_j > 1e-10:
                r = np.corrcoef(states[:, i], states[:, j])[0, 1]
                if not np.isnan(r):
                    correlations.append(abs(r))

    return float(np.mean(correlations)) if correlations else 0.5


def compute_h2_diversity(actions):
    """H2: Action distribution entropy (BEST PREDICTOR)."""
    if len(actions) < 5:
        return 0.5

    # Handle continuous actions by binning
    if actions.dtype in [np.float32, np.float64]:
        flat = actions.flatten()
        bins = np.linspace(flat.min() - 1e-10, flat.max() + 1e-10, 11)
        discretized = np.digitize(flat, bins)
    else:
        discretized = actions.flatten()

    counts = Counter(discretized)
    if len(counts) <= 1:
        return 0.0

    total = len(discretized)
    probs = np.array([c / total for c in counts.values()])
    h = -np.sum(probs * np.log(probs + 1e-10))
    h_max = np.log(len(counts))

    return float(h / h_max) if h_max > 0 else 0.0


def compute_h4_behavioral_entropy(states):
    """H4: Entropy of state value distributions."""
    if len(states) < 10:
        return 0.5

    if states.ndim == 1:
        states = states.reshape(-1, 1)

    n_bins = min(5, len(states) // 5)
    if n_bins < 2:
        return 0.5

    entropies = []
    for dim in range(min(states.shape[1], 8)):
        col = states[:, dim]
        if np.std(col) < 1e-10:
            continue

        hist, _ = np.histogram(col, bins=n_bins)
        hist = hist + 1e-10
        probs = hist / hist.sum()
        entropies.append(-np.sum(probs * np.log(probs)))

    if not entropies:
        return 0.5

    h_max = np.log(n_bins)
    return float(np.mean(entropies) / h_max) if h_max > 0 else 0.5


def compute_h5_transfer_entropy(states):
    """H5: Cross-dimension information flow."""
    if len(states) < 10 or states.ndim == 1:
        return 0.5

    if states.shape[1] < 2:
        return 0.5

    influences = []
    for i in range(min(states.shape[1], 4)):
        for j in range(i+1, min(states.shape[1], 4)):
            std_i = np.std(states[:-1, i])
            std_j = np.std(states[1:, j])

            if std_i > 1e-10 and std_j > 1e-10:
                try:
                    c_ij = abs(np.corrcoef(states[:-1, i], states[1:, j])[0, 1])
                    c_ji = abs(np.corrcoef(states[:-1, j], states[1:, i])[0, 1])

                    if not (np.isnan(c_ij) or np.isnan(c_ji)):
                        influence = 2 * min(c_ij, c_ji) / (c_ij + c_ji + 1e-10)
                        influences.append(influence)
                except:
                    pass

    return float(np.mean(influences)) if influences else 0.5


def compute_h6_flow_symmetry(observations):
    """H6: Temporal symmetry of observations."""
    if len(observations) < 6:
        return 0.5

    # Use observation norms
    if observations.ndim > 1:
        norms = np.linalg.norm(observations, axis=1)
    else:
        norms = np.abs(observations)

    mid = len(norms) // 2
    first = norms[:mid]
    second = norms[mid:mid+len(first)]

    if len(first) != len(second):
        min_len = min(len(first), len(second))
        first = first[:min_len]
        second = second[:min_len]

    # Probability distributions
    first_p = (np.abs(first) + 1e-10) / (np.abs(first).sum() + 1e-10)
    second_p = (np.abs(second) + 1e-10) / (np.abs(second).sum() + 1e-10)

    # Jensen-Shannon divergence
    m = 0.5 * (first_p + second_p)
    js = 0.5 * (
        np.sum(first_p * np.log(first_p / (m + 1e-10) + 1e-10)) +
        np.sum(second_p * np.log(second_p / (m + 1e-10) + 1e-10))
    )

    return float(1.0 - np.sqrt(np.clip(js, 0, 1)))


def analyze_episode_complete(filepath):
    """Complete analysis of one episode."""
    data = np.load(filepath, allow_pickle=True)

    result = {
        'file': filepath.name,
        'mode': 'open_loop' if 'open_loop' in filepath.name else
                'train' if 'train' in filepath.name else 'eval'
    }

    # Extract arrays
    states = np.array(data['states']) if 'states' in data else None
    actions = np.array(data['actions']) if 'actions' in data else None
    observations = np.array(data['observations']) if 'observations' in data else None
    k_values = np.array(data['K_values']).flatten() if 'K_values' in data else None

    # Simple K (from logs)
    if k_values is not None:
        result['simple_k_mean'] = float(np.mean(k_values))
        result['simple_k_std'] = float(np.std(k_values))
        result['simple_k_final'] = float(k_values[-1])

    # Compute all harmonies
    if states is not None:
        result['h1'] = compute_h1_resonant_coherence(states)
        result['h4'] = compute_h4_behavioral_entropy(states)
        result['h5'] = compute_h5_transfer_entropy(states)
        result['n_steps'] = len(states)
    else:
        result['h1'] = 0.5
        result['h4'] = 0.5
        result['h5'] = 0.5

    if actions is not None:
        result['h2'] = compute_h2_diversity(actions)
    else:
        result['h2'] = 0.5

    if observations is not None:
        result['h6'] = compute_h6_flow_symmetry(observations)
    else:
        result['h6'] = 0.5

    # H3 (prediction accuracy) and H7 (growth rate) default to neutral
    result['h3'] = 0.5
    result['h7'] = 0.0

    # Compute Full K-Index
    harmonies = [result['h1'], result['h2'], result['h3'],
                 result['h4'], result['h5'], result['h6'], result['h7']]
    result['full_k'] = float(np.mean(harmonies))

    return result


def compute_statistics(values):
    """Compute publication-ready statistics."""
    arr = np.array(values)
    n = len(arr)

    mean = float(np.mean(arr))
    std = float(np.std(arr, ddof=1)) if n > 1 else 0.0
    sem = std / np.sqrt(n) if n > 0 else 0.0

    # 95% confidence interval
    if n > 1:
        ci = stats.t.interval(0.95, n-1, loc=mean, scale=sem)
        ci_low, ci_high = float(ci[0]), float(ci[1])
    else:
        ci_low, ci_high = mean, mean

    return {
        'mean': mean,
        'std': std,
        'sem': sem,
        'ci_95_low': ci_low,
        'ci_95_high': ci_high,
        'n': n,
        'values': [float(v) for v in arr]
    }


def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ðŸ“Š Paper 1: Complete Track B Full K-Index Analysis           â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

    log_dir = Path('logs')
    npz_files = sorted(log_dir.glob('track_b_*.npz'))

    if not npz_files:
        print("No Track B .npz files found!")
        return

    print(f"Analyzing {len(npz_files)} episodes...\n")

    # Analyze all episodes
    open_loop = []
    controller_eval = []

    for filepath in npz_files:
        result = analyze_episode_complete(filepath)

        if result['mode'] == 'open_loop':
            open_loop.append(result)
        elif result['mode'] == 'eval':
            controller_eval.append(result)

    print(f"Open Loop episodes: {len(open_loop)}")
    print(f"Controller Eval episodes: {len(controller_eval)}\n")

    # Compute statistics for each metric
    metrics = ['simple_k_mean', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'full_k']

    results = {
        'open_loop': {},
        'controller': {},
        'comparison': {},
        'metadata': {
            'date': datetime.now().isoformat(),
            'n_open_loop': len(open_loop),
            'n_controller': len(controller_eval),
            'total_episodes': len(npz_files)
        }
    }

    print("=" * 70)
    print("\nðŸ“ˆ PUBLICATION-READY RESULTS\n")
    print("Metric        â”‚ Open Loop (meanÂ±SEM)  â”‚ Controller (meanÂ±SEM) â”‚ Change")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€")

    for metric in metrics:
        ol_vals = [ep[metric] for ep in open_loop if metric in ep]
        ce_vals = [ep[metric] for ep in controller_eval if metric in ep]

        if ol_vals and ce_vals:
            ol_stats = compute_statistics(ol_vals)
            ce_stats = compute_statistics(ce_vals)

            results['open_loop'][metric] = ol_stats
            results['controller'][metric] = ce_stats

            # Statistical test
            if len(ol_vals) > 1 and len(ce_vals) > 1:
                t_stat, p_val = stats.ttest_ind(ol_vals, ce_vals)

                # Effect size (Cohen's d)
                pooled_std = np.sqrt(
                    ((len(ol_vals)-1)*ol_stats['std']**2 + (len(ce_vals)-1)*ce_stats['std']**2) /
                    (len(ol_vals) + len(ce_vals) - 2)
                )
                cohens_d = (ce_stats['mean'] - ol_stats['mean']) / pooled_std if pooled_std > 0 else 0
            else:
                t_stat, p_val = 0, 1
                cohens_d = 0

            # Percent change
            if ol_stats['mean'] > 0:
                pct_change = ((ce_stats['mean'] / ol_stats['mean']) - 1) * 100
            else:
                pct_change = float('inf') if ce_stats['mean'] > 0 else 0

            results['comparison'][metric] = {
                'percent_change': float(pct_change),
                't_statistic': float(t_stat),
                'p_value': float(p_val),
                'cohens_d': float(cohens_d),
                'significant': p_val < 0.05
            }

            # Print
            sig = '*' if p_val < 0.05 else ''
            if pct_change == float('inf'):
                change_str = "+âˆž%"
            else:
                change_str = f"{pct_change:+.1f}%{sig}"

            print(f"{metric:13s} â”‚ {ol_stats['mean']:.4f} Â± {ol_stats['sem']:.4f}    â”‚ "
                  f"{ce_stats['mean']:.4f} Â± {ce_stats['sem']:.4f}    â”‚ {change_str}")

    print("\n* p < 0.05\n")

    # Key findings for paper
    print("=" * 70)
    print("\nðŸ”‘ KEY FINDINGS FOR PAPER 1\n")

    # Simple K
    sk_ol = results['open_loop']['simple_k_mean']['mean']
    sk_ce = results['controller']['simple_k_mean']['mean']
    sk_change = results['comparison']['simple_k_mean']['percent_change']
    print(f"1. Simple K-Index:")
    print(f"   Open Loop: {sk_ol:.4f}, Controller: {sk_ce:.4f}")
    print(f"   Change: {sk_change:+.1f}% (NOT the claimed 63%)")

    # H2 (Diversity)
    h2_ol = results['open_loop']['h2']['mean']
    h2_ce = results['controller']['h2']['mean']
    h2_p = results['comparison']['h2']['p_value']
    print(f"\n2. H2 (Diversity) - BEST PREDICTOR:")
    print(f"   Open Loop: {h2_ol:.4f}, Controller: {h2_ce:.4f}")
    print(f"   Controller learned DIVERSE actions (p = {h2_p:.4f})")

    # Full K
    fk_ol = results['open_loop']['full_k']['mean']
    fk_ce = results['controller']['full_k']['mean']
    fk_change = results['comparison']['full_k']['percent_change']
    fk_p = results['comparison']['full_k']['p_value']
    fk_d = results['comparison']['full_k']['cohens_d']
    print(f"\n3. Full K-Index:")
    print(f"   Open Loop: {fk_ol:.4f}, Controller: {fk_ce:.4f}")
    print(f"   Change: {fk_change:+.1f}%")
    print(f"   p = {fk_p:.4f}, Cohen's d = {fk_d:.2f}")

    # Summary sentence for abstract
    print("\n" + "=" * 70)
    print("\nðŸ“ SUGGESTED ABSTRACT TEXT:\n")
    print(f'"The SAC controller achieved a {fk_change:.1f}% improvement in Full K-Index')
    print(f'(open loop: {fk_ol:.3f}, controller: {fk_ce:.3f}), driven primarily by')
    print(f'a substantial increase in action diversity (H2: {h2_ol:.2f} â†’ {h2_ce:.2f})."')

    # Save results
    output_path = Path('logs/paper1_track_b_analysis.json')
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nðŸ“ Full results saved to: {output_path}")
    print("\nâœ… Analysis complete - ready for Paper 1 revision!\n")


if __name__ == '__main__':
    main()
