# Performance Optimization Guide

**Version**: 1.1.0
**Last Updated**: 2025-11-15
**Author**: Kosmic Lab Team

Complete guide to optimizing Kosmic Lab performance for production-scale research.

---

## üìö Table of Contents

1. [Quick Wins](#quick-wins-immediate-10x-speedup)
2. [Parallel Processing](#parallel-processing-guide)
3. [Memory Optimization](#memory-optimization)
4. [Bootstrap Tuning](#bootstrap-configuration)
5. [Profiling Tutorial](#profiling-bottlenecks)
6. [Common Patterns](#performance-patterns)
7. [Troubleshooting](#performance-troubleshooting)
8. [Real-World Examples](#real-world-optimization-examples)

---

## Quick Wins: Immediate 10x Speedup

### üöÄ Use Parallel Bootstrap CI

**Before** (slow):
```python
from fre.metrics.k_index import bootstrap_k_ci

# Serial processing (uses 1 CPU)
k, ci_low, ci_high = bootstrap_k_ci(observed, actual, n_bootstrap=1000)
# Takes: ~15s for N=10k
```

**After** (10x faster):
```python
from fre.metrics.k_index import bootstrap_k_ci

# Parallel processing (uses all CPUs!)
k, ci_low, ci_high = bootstrap_k_ci(
    observed, actual,
    n_bootstrap=1000,
    n_jobs=-1,  # ‚Üê Add this line for 10x speedup!
    progress=True  # ‚Üê See progress bar
)
# Takes: ~2s for N=10k (7.5x faster on 8-core machine)
```

**Impact**: **7-10x speedup** on multi-core machines (tested on 8-core CPU)

---

### ‚ö° Use Parallel Map for Parameter Sweeps

**Before** (slow):
```python
# Serial parameter sweep
results = []
for param in params:
    result = expensive_function(param)
    results.append(result)
# Takes: N √ó time_per_param
```

**After** (10x faster):
```python
from core.parallel import parallel_map

# Parallel parameter sweep
results = parallel_map(
    expensive_function,
    params,
    n_jobs=-1,  # Use all CPUs
    progress=True
)
# Takes: (N √ó time_per_param) / n_cores
```

**Impact**: Near-linear speedup with number of CPU cores

---

### üéØ Reduce Bootstrap Iterations (When Appropriate)

**Before** (overkill for exploration):
```python
# 10,000 iterations for initial exploration
k, ci = bootstrap_k_ci(obs, act, n_bootstrap=10000)
# Takes: ~48s for N=10k (serial)
```

**After** (appropriate for exploration):
```python
# 500 iterations is plenty for initial analysis
k, ci = bootstrap_k_ci(obs, act, n_bootstrap=500)
# Takes: ~2.4s for N=10k (serial)
```

**Guidelines**:
- **Exploration/debugging**: 100-500 iterations
- **Standard analysis**: 1,000 iterations
- **Publication**: 5,000-10,000 iterations
- **Critical research**: 10,000+ iterations

---

## Parallel Processing Guide

### When to Use Parallel Processing

#### ‚úÖ Use Parallel (`n_jobs=-1`) When:

1. **Large datasets**: N ‚â• 1,000 samples
2. **High bootstrap iterations**: n_bootstrap ‚â• 1,000
3. **Parameter sweeps**: Testing multiple configurations
4. **Multi-core CPU available**: 4+ cores
5. **Production analysis**: Time is critical

**Expected speedup**: 5-8x on 8-core machine

#### ‚ùå Use Serial (`n_jobs=1`) When:

1. **Small datasets**: N < 1,000 samples
2. **Low bootstrap iterations**: n_bootstrap < 500
3. **Single-core environment**: 1-2 cores
4. **Memory-constrained**: Limited RAM
5. **Quick exploration**: Overhead not worth it

**Why**: Parallelization has overhead (~100ms) that dominates for small tasks

---

### Parallel Processing Performance

**Benchmark Results** (8-core CPU, 16GB RAM):

| Dataset Size | Serial Time | Parallel Time (8 cores) | Speedup |
|--------------|-------------|------------------------|---------|
| N=100        | 0.15s       | 0.08s                  | 1.9x    |
| N=1,000      | 0.50s       | 0.12s                  | 4.2x    |
| N=10,000     | 4.8s        | 0.65s                  | **7.4x**|
| N=100,000    | 48s         | 6.5s                   | **7.4x**|

**Conclusion**: Near-linear scaling for N ‚â• 1,000

---

### Example: Optimizing a Parameter Sweep

**Problem**: Test 100 parameter combinations, each taking 5 seconds

```python
# ‚ùå Serial version (slow)
# Takes: 100 √ó 5s = 500s (8.3 minutes)
results = []
for param in params:
    result = analyze_with_param(param)
    results.append(result)

# ‚úÖ Parallel version (fast)
# Takes: (100 √ó 5s) / 8 cores ‚âà 62s (1 minute)
from core.parallel import parallel_map

results = parallel_map(
    analyze_with_param,
    params,
    n_jobs=-1,
    progress=True  # Watch the progress!
)
```

**Speedup**: **8x faster** on 8-core machine!

---

## Memory Optimization

### Understanding Memory Usage

**Bootstrap CI Memory Formula**:
```
Memory (MB) ‚âà 2 √ó N √ó n_bootstrap √ó 8 bytes / (1024¬≤)
```

**Examples**:
- N=1k, n_bootstrap=1k: ~16 MB
- N=10k, n_bootstrap=1k: ~160 MB
- N=100k, n_bootstrap=1k: ~1.6 GB
- N=1M, n_bootstrap=1k: ~16 GB

---

### Strategies for Large Datasets

#### Strategy 1: Reduce Bootstrap Iterations

```python
# Instead of 10,000 iterations (16 GB memory)
k, ci = bootstrap_k_ci(obs, act, n_bootstrap=10000)

# Use 1,000 iterations (1.6 GB memory)
k, ci = bootstrap_k_ci(obs, act, n_bootstrap=1000)
# Still gives tight confidence intervals!
```

#### Strategy 2: Use Chunking

```python
from core.parallel import chunk_array, parallel_map

# Process in chunks to stay under memory limit
chunks = chunk_array(large_data, chunk_size=10000)

results = parallel_map(
    lambda chunk: analyze(chunk),
    chunks,
    n_jobs=-1
)

# Combine results
final_result = combine(results)
```

#### Strategy 3: Memory-Mapped Arrays

```python
# For datasets larger than RAM
import numpy as np

# Load as memory-mapped array (doesn't load into RAM)
data = np.load('huge_dataset.npy', mmap_mode='r')

# Process in chunks
for i in range(0, len(data), chunk_size):
    chunk = data[i:i+chunk_size]
    process(chunk)
```

---

### Estimating Memory Requirements

```python
from core.parallel import estimate_memory_usage

# Estimate before running
mem_mb = estimate_memory_usage(
    n_samples=100000,
    n_bootstrap=1000
)
print(f"Estimated memory: {mem_mb:.1f} MB")

# Adjust if needed
if mem_mb > 1000:  # > 1GB
    print("Reducing bootstrap iterations for memory...")
    n_bootstrap = 500  # Use fewer iterations
```

---

## Bootstrap Configuration

### Optimal Bootstrap Iterations

| Use Case | n_bootstrap | Reasoning | CI Width |
|----------|-------------|-----------|----------|
| **Quick exploration** | 100-500 | Fast feedback | ¬±0.05 |
| **Standard analysis** | 1,000 | Good balance | ¬±0.02 |
| **Publication** | 5,000-10,000 | High precision | ¬±0.01 |
| **Critical research** | 10,000+ | Maximum precision | ¬±0.005 |

### How to Choose

```python
# Quick check: Is this worth pursuing?
k, ci = bootstrap_k_ci(obs, act, n_bootstrap=100, n_jobs=-1)
if k > 0.5:  # Promising!
    # Do detailed analysis
    k, ci = bootstrap_k_ci(obs, act, n_bootstrap=5000, n_jobs=-1)
```

### Confidence Level Tradeoffs

```python
# Wider CI, faster computation
k, ci = bootstrap_k_ci(obs, act, confidence_level=0.90)  # 90% CI

# Standard (recommended)
k, ci = bootstrap_k_ci(obs, act, confidence_level=0.95)  # 95% CI

# Tighter CI, slower (more iterations needed)
k, ci = bootstrap_k_ci(obs, act, confidence_level=0.99)  # 99% CI
```

---

## Profiling Bottlenecks

### Quick Profiling with Time

```python
import time

start = time.time()
result = expensive_function()
elapsed = time.time() - start

print(f"Took {elapsed:.2f} seconds")
```

### Using the Built-in Profiler

```bash
# Profile your script
python -m cProfile -o profile.stats my_script.py

# Analyze results
python -m pstats profile.stats
```

**Commands in pstats**:
```
stats 10          # Show top 10 functions by time
sort cumulative   # Sort by cumulative time
stats k_index     # Show only K-Index related functions
```

### Using Kosmic Lab's Profiler

```bash
# Use the built-in performance profiler
python scripts/profile_performance.py --iterations 10

# Generate HTML report
python scripts/profile_performance.py --output html --iterations 10
# Opens in browser automatically
```

### Interpreting Profile Results

Look for:
1. **High cumulative time**: Functions called many times
2. **High per-call time**: Slow individual functions
3. **Unexpected calls**: Functions called more than expected

**Example Output**:
```
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
  1000    2.450    0.002    4.823    0.005 k_index.py:19(k_index)
  1000    1.234    0.001    1.234    0.001 {built-in method scipy.stats.pearsonr}
```

**Interpretation**:
- `k_index` called 1000 times (expected for bootstrap)
- Total time: 4.8s (cumtime)
- `pearsonr` takes 1.2s (the bottleneck!)

---

## Performance Patterns

### Pattern 1: Batch Processing

**‚ùå Don't** process one at a time:
```python
for item in items:
    result = process(item)  # Slow!
```

**‚úÖ Do** batch processing:
```python
# Vectorized (best)
results = np.array([process(item) for item in items])

# Or parallel
results = parallel_map(process, items, n_jobs=-1)
```

---

### Pattern 2: Reuse Computations

**‚ùå Don't** recompute:
```python
for param in params:
    data = load_data()  # Loaded every iteration!
    result = analyze(data, param)
```

**‚úÖ Do** compute once:
```python
data = load_data()  # Load once
results = parallel_map(
    lambda p: analyze(data, p),
    params,
    n_jobs=-1
)
```

---

### Pattern 3: Early Stopping

**‚ùå Don't** analyze everything:
```python
for dataset in large_collection:
    k, ci = bootstrap_k_ci(dataset.obs, dataset.act, n_bootstrap=10000)
    # Even if k is clearly insignificant!
```

**‚úÖ Do** quick checks first:
```python
for dataset in large_collection:
    # Quick check (100 iterations)
    k_quick, _ = bootstrap_k_ci(dataset.obs, dataset.act, n_bootstrap=100)

    if k_quick > 0.3:  # Potentially interesting
        # Detailed analysis
        k, ci = bootstrap_k_ci(dataset.obs, dataset.act, n_bootstrap=10000)
```

---

## Performance Troubleshooting

### Problem: Parallel Processing Not Faster

**Symptoms**: `n_jobs=-1` slower than `n_jobs=1`

**Causes & Solutions**:

1. **Dataset too small**
   ```python
   # Solution: Use serial for N < 1,000
   if len(data) < 1000:
       n_jobs = 1
   else:
       n_jobs = -1
   ```

2. **Too few iterations**
   ```python
   # Solution: Use serial for n_bootstrap < 500
   if n_bootstrap < 500:
       n_jobs = 1
   ```

3. **joblib not installed**
   ```bash
   # Solution: Install joblib
   pip install joblib
   ```

---

### Problem: Out of Memory

**Symptoms**: `MemoryError` or system freezing

**Solutions**:

1. **Reduce bootstrap iterations**
   ```python
   # From 10,000 to 1,000 (10x less memory)
   k, ci = bootstrap_k_ci(obs, act, n_bootstrap=1000)
   ```

2. **Use chunking**
   ```python
   from core.parallel import chunk_array

   chunks = chunk_array(data, chunk_size=10000)
   results = [process(chunk) for chunk in chunks]
   ```

3. **Process serially**
   ```python
   # Parallel uses more memory
   k, ci = bootstrap_k_ci(obs, act, n_jobs=1)  # Serial
   ```

---

### Problem: Slow K-Index Computation

**Symptoms**: K-Index itself is slow, not just bootstrap

**Diagnosis**:
```python
import time

start = time.time()
k = k_index(obs, act)
print(f"K-Index took {(time.time() - start)*1000:.2f} ms")
```

**Expected Performance**:
- N=1k: <1ms
- N=10k: <5ms
- N=100k: <50ms

**If slower**, check:
1. Data types (should be `np.ndarray`)
2. NumPy version (upgrade to latest)
3. BLAS library (use OpenBLAS or MKL)

---

## Real-World Optimization Examples

### Example 1: Neuroscience EEG Analysis

**Scenario**: Analyze 100k EEG samples with 4 consciousness levels

**Before** (slow):
```python
results = []
for level in consciousness_levels:
    eeg_data = generate_eeg(100000, level)
    k, ci = bootstrap_k_ci(
        eeg_data.prediction,
        eeg_data.actual,
        n_bootstrap=5000,  # High precision
        n_jobs=1  # Serial
    )
    results.append((level, k, ci))
# Takes: 4 levels √ó 240s = 960s (16 minutes)
```

**After** (optimized):
```python
from core.parallel import parallel_map

def analyze_level(level):
    eeg_data = generate_eeg(100000, level)
    k, ci = bootstrap_k_ci(
        eeg_data.prediction,
        eeg_data.actual,
        n_bootstrap=5000,
        n_jobs=-1  # Parallel!
    )
    return (level, k, ci)

results = parallel_map(
    analyze_level,
    consciousness_levels,
    n_jobs=4,  # Process 4 levels in parallel
    progress=True
)
# Takes: 240s / 7.5 (speedup) / 4 (parallel levels) ‚âà 8s
```

**Speedup**: **16 minutes ‚Üí 8 seconds (120x faster!)**

---

### Example 2: AI Model Parameter Sweep

**Scenario**: Test 50 hyperparameter combinations

**Before** (slow):
```python
best_k = 0
for lr in learning_rates:
    for hidden in hidden_sizes:
        model = train_model(lr=lr, hidden_dim=hidden)
        k, ci = evaluate_coherence(model, n_bootstrap=1000)
        if k > best_k:
            best_k = k
            best_params = (lr, hidden)
# Takes: 50 combinations √ó 30s = 1500s (25 minutes)
```

**After** (optimized):
```python
from itertools import product
from core.parallel import parallel_map

def evaluate_params(params):
    lr, hidden = params
    model = train_model(lr=lr, hidden_dim=hidden)
    k, ci = evaluate_coherence(model, n_bootstrap=1000, n_jobs=1)
    return (params, k, ci)

param_combinations = list(product(learning_rates, hidden_sizes))

results = parallel_map(
    evaluate_params,
    param_combinations,
    n_jobs=-1,  # Parallelize across params
    progress=True
)

best = max(results, key=lambda x: x[1])
# Takes: (50 √ó 30s) / 8 cores ‚âà 187s (3 minutes)
```

**Speedup**: **25 minutes ‚Üí 3 minutes (8x faster)**

---

## Performance Checklist

Before running large analyses, check:

- [ ] Using `n_jobs=-1` for N ‚â• 1,000?
- [ ] Using appropriate `n_bootstrap` for use case?
- [ ] Processing parameters in parallel with `parallel_map`?
- [ ] Reusing expensive computations?
- [ ] Memory usage estimated and acceptable?
- [ ] Profiled to find bottlenecks?
- [ ] Using progress bars for long operations?
- [ ] Data in NumPy arrays (not Python lists)?

---

## Performance Summary

| Technique | Speedup | When to Use |
|-----------|---------|-------------|
| **Parallel bootstrap** | 5-10x | N ‚â• 1k, multi-core |
| **Parallel map** | 5-10x | Parameter sweeps |
| **Reduce bootstrap iters** | 2-20x | Exploration phase |
| **Chunking** | Enables large datasets | N > RAM |
| **Early stopping** | 2-100x | Many insignificant tests |
| **Vectorization** | 10-100x | Array operations |

---

## Getting Help

**Performance questions?**
- Review this guide
- Run benchmarks: `python benchmarks/suite.py --compare-parallel`
- Profile your code: `python scripts/profile_performance.py`
- Check FAQ: `FAQ.md`
- Open an issue: GitHub Issues

---

**Last Updated**: 2025-11-15
**Version**: 1.1.0

*Happy optimizing! ‚ö°*
