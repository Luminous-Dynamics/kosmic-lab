# Track F: Adversarial Robustness Testing
# Tests K-Index stability under various perturbation types

experiment:
  name: "track_f_adversarial_robustness"
  n_episodes: 30  # Per condition (5 conditions Ã— 30 = 150 total)
  save_interval: 5
  description: "Test coherence robustness under adversarial perturbations"

environment:
  type: "adversarial"
  obs_dim: 20
  action_dim: 10
  max_steps: 300
  base_difficulty: 2.0  # Moderate baseline difficulty

agent:
  type: "robust_learner"
  architecture: "TD3"  # Twin Delayed Deep Deterministic Policy Gradient
  learning_rate: 0.001
  discount_factor: 0.99
  exploration_noise: 0.1

  # Robustness mechanisms (optional, tested in conditions)
  adversarial_training: false  # Train on perturbed data
  defensive_distillation: false  # Smooth decision boundaries
  randomized_smoothing: false  # Add defensive noise

# Adversarial conditions to test
adversarial_conditions:
  # Condition 1: Baseline (no perturbations)
  - name: "baseline"
    perturbation_type: "none"
    perturbation_strength: 0.0
    perturbation_frequency: 0.0
    description: "Clean environment, no adversarial attacks"

  # Condition 2: Observation noise
  - name: "observation_noise"
    perturbation_type: "gaussian_noise"
    perturbation_strength: 0.3  # Noise std relative to signal
    perturbation_frequency: 1.0  # Every timestep
    target: "observations"
    description: "Gaussian noise added to observations"

  # Condition 3: Action interference
  - name: "action_interference"
    perturbation_type: "random_flip"
    perturbation_strength: 0.2  # 20% of action dimensions flipped
    perturbation_frequency: 0.3  # 30% of timesteps
    target: "actions"
    description: "Random action components perturbed"

  # Condition 4: Reward spoofing
  - name: "reward_spoofing"
    perturbation_type: "sign_flip"
    perturbation_strength: 0.5  # 50% chance of sign flip
    perturbation_frequency: 0.2  # 20% of timesteps
    target: "rewards"
    description: "Reward signals occasionally inverted"

  # Condition 5: Adversarial examples (FGSM-style)
  - name: "adversarial_examples"
    perturbation_type: "gradient_based"
    perturbation_strength: 0.15  # Epsilon for perturbation
    perturbation_frequency: 0.5  # 50% of timesteps
    target: "observations"
    description: "Gradient-based adversarial perturbations"

# Metrics to track
metrics:
  - "k_index"  # Primary consciousness metric
  - "k_index_variance"  # Stability of coherence
  - "recovery_time"  # Time to recover from perturbation
  - "baseline_performance_ratio"  # K relative to clean baseline
  - "episode_reward"  # Task performance under attack

# Recovery analysis
recovery:
  measure_recovery: true
  recovery_threshold: 0.9  # 90% of baseline K-Index
  perturbation_duration: 50  # Steps of perturbation
  recovery_window: 100  # Steps to measure recovery

output:
  save_path: "logs/track_f/adversarial"
  format: "npz"
  save_history: true
  generate_figures: true
